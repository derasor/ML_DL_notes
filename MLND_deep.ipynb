{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and code used on my UDACITY MLND Deep Learning module\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "Code for implementing Sigmoid function and Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.999999994397\n",
      "8.31528027664e-07\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SIGMOID FUNCTION\n",
    "'''\n",
    "\n",
    "def sigmoid_f(x):\n",
    "    if x < 0:\n",
    "        z = np.exp(x)\n",
    "        return z / (1+z)\n",
    "    elif x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1+z)\n",
    "\n",
    "def score(x_1, x_2):\n",
    "    return 4*x_1 + 5*x_2 - 9\n",
    "\n",
    "print(sigmoid_f(score(1,1)))\n",
    "print(sigmoid_f(score(2,4)))\n",
    "print(sigmoid_f(score(5,-5)))\n",
    "print(sigmoid_f(score(-4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6652409557748219, 0.24472847105479764, 0.090030573170380462]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SOFTMAX FUNCTION\n",
    "'''\n",
    "def softmax(L):\n",
    "    eL = []\n",
    "    softmaxL = []\n",
    "    for e in L:\n",
    "        eL.append(np.exp(e))\n",
    "    sumeL = sum(eL)\n",
    "    for v in eL:\n",
    "        softmaxL.append(v/float(sumeL))\n",
    "    return softmaxL\n",
    "\n",
    "L = [2, 1, 0]\n",
    "print(softmax(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09003057  0.24472847  0.66524096]\n"
     ]
    }
   ],
   "source": [
    "'''Better implementation of SOFTMAX'''\n",
    "def softmax_o(L):\n",
    "    expL = np.exp(L)\n",
    "    return np.divide (expL, expL.sum())\n",
    "\n",
    "L = [5, 6, 7]\n",
    "print(softmax_o(L))\n",
    "# [0.090030573170380462, 0.24472847105479764, 0.6652409557748219]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The score function represent the line (plane - hyperplane) of an arbitrary neural network. The sigmoid function is a continuous differentiable function that gives back a probability of a given point to be on one or the other side of the classification boundary. A pribability of 0.5 is equivalent to having the given point right on the boundary.\n",
    "\n",
    "The softmax function transforms a list of values to a list of probabilities that add up to one. In the case of having more than two classes, the softmax function gives the probabilities of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross - Entropy\n",
    "\n",
    "If we have probabilities for each data point, we could multiply probabilities of all datapoints to get a value for a model. Since this is not a good approach because the numbers will be very small for several datapoints, we use the logarithmic property that states that the log of a multiplication is just the addition of the log of each of its constituents. Using natural log, which is negative for numbers between 0 and 1 (ln 0 approaches -inf and ln 1 = 0) we would get negative numbers, so we use a the negative of the natural log to get positive numbers and thus the expression ends up being:\n",
    "\n",
    "-Σ[i=1, m] y_i \\* ln(p_i) + (1-y_i) \\* ln(1-p_i)\n",
    "\n",
    "The smaller the cross entropy, the better the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8283137373\n"
     ]
    }
   ],
   "source": [
    "'''CROSS ENTROPY'''\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    result = 0\n",
    "    for i in range(len(Y)):\n",
    "        result -=  Y[i]*np.log(P[i])+(1-Y[i])*np.log(1-P[i])\n",
    "    return result\n",
    "\n",
    "Y=[1,0,1,1]\n",
    "P=[0.4,0.6,0.1,0.5]\n",
    "print(cross_entropy(Y,P))\n",
    "# 4.8283137373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Provided solution'''\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the y_i is either one or zero, and there is only one vector of ys, so this is equivalent of having two classes. If we have multiple classes (n of them), the cross entropy is the following:\n",
    "\n",
    "-Σ[i=1, m] Σ[j=1, n] y_i_j \\* ln(p_i_j) \n",
    "\n",
    "y_i_j is 0 or 1, thus we are only adding the events that actually occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "\n",
    "The error function for one point is:\n",
    "\n",
    "-(1-y)\\*ln(1-y_pred) - y\\*ln(y_pred)\n",
    "\n",
    "if y is equal to 1, only the second term remains; if zero only the first term remains. y_pred is the prediction, or probability, of the datapoint being of class '1' [y_pred = p(y=1)] -> according to the model.\n",
    "\n",
    "The error function for the model then is the average of the errors:\n",
    "\n",
    "-1/m \\*  Σ[i=1, m] (1-y_i)\\*ln(1-y_pred_i) + y_i\\*ln(y_pred_i)\n",
    "\n",
    "or, since y_pred = f_act(Wx+b)\n",
    "\n",
    "-1/m \\*  Σ[i=1, m] (1-y_i)\\*ln(1-f_act(Wx_i+b)) + y_i\\*ln(f_act(Wx_i+b))\n",
    "\n",
    "Generalizing for more than two classes:\n",
    "\n",
    "-1/m \\*  Σ[i=1, m] Σ[j=1, n] y_i_j \\* ln(y_pred_i_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
