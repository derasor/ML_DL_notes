{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Use weights in pre-trained Conv Net (mostly from initail layers - general pattern detection), and adapt it (changing last more specific layers) to a new dataset.\n",
    "\n",
    "The changes depend on:\n",
    "\n",
    "* Size of new dataset (large in the hundred thousands to millions, small in the thousands to tens of thousands). Overfitting is a concern when using small datasets in transfer learning.\n",
    "\n",
    "* Similarity of new dataset with original data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New data set is small. New data is similar to original training data\n",
    "\n",
    "* Slice off the end of the neural network (last fully connected layer).\n",
    "* Add a new fully connected layer that matches the number of classes in the new data set.\n",
    "* Randomize the weights of the new fully connected layer; freeze all the weights from the pre-trained network.\n",
    "* Train the network to update the weights of the new fully connected layer.\n",
    "\n",
    "To avoid overfitting on the small data set, the weights of the original network will be held constant rather than re-training the weights.\n",
    "\n",
    "Since the data sets are similar, images from each data set will have similar higher level features. Therefore most or all of the pre-trained neural network layers already contain relevant information about the new data set and should be kept.\n",
    "\n",
    "\n",
    "\n",
    "### New data set is small. New data is different from original training data\n",
    "\n",
    "* Slice off last conv layers (higher level features), an all fully connected layers.\n",
    "* Add to the remaining pre-trained layers (lower conv layers identifying edges and simple shapes) a new fully connected layer that matches the number of classes in the new data set.\n",
    "* Randomize the weights of the new fully connected layer; freeze all the weights from the pre-trained network.\n",
    "* Train the network to update the weights of the new fully connected layer.\n",
    "\n",
    "Because the dataset is small, overfitting is still a concern. To combat overfitting, the weights of the original neural network will be held constant, like in the first case.\n",
    "\n",
    "But the original training set and the new data set do not share higher level features. In this case, the new network will only use the layers containing lower level features.\n",
    "\n",
    "### New data set is large. New data is similar to original training data\n",
    "\n",
    "* Remove the last fully connected layer and replace with a fully connected layer matching the number of classes in the new dataset.\n",
    "* Randomly initialize the weights in the new fully connected layer.\n",
    "* **Initialize** the rest of the weights using the pre-trained weights.\n",
    "* Re-train the entire neural network.\n",
    "\n",
    "Overfitting is not as much of a concern when training on a large data set; therefore, you can re-train all of the weights.\n",
    "\n",
    "Because the original training set and the new data set share higher level features, the entire neural network is used as well.\n",
    "\n",
    "\n",
    "### New data set is large. New data is different from original training data\n",
    "\n",
    "* Remove the last fully connected layer and replace with a layer matching the number of classes in the new data set.\n",
    "* Retrain the network from scratch with randomly initialized weights.\n",
    "* **Alternatively**, you could just use the same strategy as the \"large and similar\" data case.\n",
    "\n",
    "Even though the data set is different from the training data, initializing the weights from the pre-trained network might make training faster. So this case is exactly the same as the case with a large, similar data set.\n",
    "\n",
    "If using the pre-trained network as a starting point does not produce a successful model, another option is to randomly initialize the convolutional neural network weights and train the network from scratch.\n",
    "\n",
    "\n",
    "Ref:\n",
    "\n",
    "[How Transferable are Features in Deep Neural Networks - paper](https://arxiv.org/pdf/1411.1792.pdf)\n",
    "\n",
    "Optional resources:\n",
    "\n",
    "[Learning Deep Features for Discriminative Localization - GAP layers for object localization](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)\n",
    "\n",
    "[ResNetCAM-keras GitHub repo - Conv Nets for object localization](https://github.com/alexisbcook/ResNetCAM-keras)\n",
    "\n",
    "[keras_transfer_cifar10 GitHub repo - visualization techniques to better understand bottleneck features](https://github.com/alexisbcook/keras_transfer_cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning in Keras\n",
    "\n",
    "Using `tranfer-learning/` folder in `udacity/machine-learning/projects/practice_projects/cnn/` GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
