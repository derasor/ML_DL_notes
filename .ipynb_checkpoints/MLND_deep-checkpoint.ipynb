{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and code used on my UDACITY MLND Deep Learning module\n",
    "\n",
    "These notes are not complete and are just a personal reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron algo\n",
    "\n",
    "This is the code. Needs data.csv and other code that runs under the hood, so I won't run it and just add it for reference.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    ##return stepFunction((np.matmul(X,W)+b)[0])\n",
    "    return [stepFunction(e) for e in np.matmul(X,W)+b]\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    y_pred = prediction(X[i], W, b)\n",
    "    for i in range(len(X)):\n",
    "        ##y_pred = prediction(X[i], W, b)\n",
    "        if y_pred[i] > y[i]:\n",
    "        ##if y_pred > y[i]:\n",
    "            for n in range(len(W)):\n",
    "                W[n] -= learn_rate*X[i][n]\n",
    "            b -= learn_rate\n",
    "        elif y_pred[i] < y[i]:\n",
    "        ##elif y_pred < y[i]:\n",
    "            for n in range(len(W)):\n",
    "                W[n] += learn_rate*X[i][n]\n",
    "            b += learn_rate\n",
    "    return W, b\n",
    "\n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Code for implementing Sigmoid function and Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.999999994397\n",
      "8.31528027664e-07\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SIGMOID FUNCTION\n",
    "'''\n",
    "\n",
    "def sigmoid_f(x):\n",
    "    if x < 0:\n",
    "        z = np.exp(x)\n",
    "        return z / (1+z)\n",
    "    elif x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1+z)\n",
    "\n",
    "def score(x_1, x_2):\n",
    "    return 4*x_1 + 5*x_2 - 9\n",
    "\n",
    "print(sigmoid_f(score(1,1)))\n",
    "print(sigmoid_f(score(2,4)))\n",
    "print(sigmoid_f(score(5,-5)))\n",
    "print(sigmoid_f(score(-4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6652409557748219, 0.24472847105479764, 0.090030573170380462]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SOFTMAX FUNCTION\n",
    "'''\n",
    "def softmax(L):\n",
    "    eL = []\n",
    "    softmaxL = []\n",
    "    for e in L:\n",
    "        eL.append(np.exp(e))\n",
    "    sumeL = sum(eL)\n",
    "    for v in eL:\n",
    "        softmaxL.append(v/float(sumeL))\n",
    "    return softmaxL\n",
    "\n",
    "L = [2, 1, 0]\n",
    "print(softmax(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09003057  0.24472847  0.66524096]\n"
     ]
    }
   ],
   "source": [
    "'''Better implementation of SOFTMAX'''\n",
    "def softmax_o(L):\n",
    "    expL = np.exp(L)\n",
    "    return np.divide (expL, expL.sum())\n",
    "\n",
    "L = [5, 6, 7]\n",
    "print(softmax_o(L))\n",
    "# [0.090030573170380462, 0.24472847105479764, 0.6652409557748219]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The score function represent the line (plane - hyperplane) of an arbitrary neural network. The sigmoid function is a continuous differentiable function that gives back a probability of a given point to be on one or the other side of the classification boundary. A pribability of 0.5 is equivalent to having the given point right on the boundary.\n",
    "\n",
    "The softmax function transforms a list of values to a list of probabilities that add up to one. In the case of having more than two classes, the softmax function gives the probabilities of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross - Entropy\n",
    "\n",
    "If we have probabilities for each data point, we could multiply probabilities of all datapoints to get a value for a model. Since this is not a good approach because the numbers will be very small for several datapoints, we use the logarithmic property that states that the log of a multiplication is just the addition of the log of each of its constituents. Using natural log, which is negative for numbers between 0 and 1 (ln 0 approaches -inf and ln 1 = 0) we would get negative numbers, so we use a the negative of the natural log to get positive numbers and thus the expression ends up being:\n",
    "\n",
    "-Σ[i=1, m] y_i \\* ln(p_i) + (1-y_i) \\* ln(1-p_i)\n",
    "\n",
    "The smaller the cross entropy, the better the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8283137373\n"
     ]
    }
   ],
   "source": [
    "'''CROSS ENTROPY'''\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    result = 0\n",
    "    for i in range(len(Y)):\n",
    "        result -=  Y[i]*np.log(P[i])+(1-Y[i])*np.log(1-P[i])\n",
    "    return result\n",
    "\n",
    "Y=[1,0,1,1]\n",
    "P=[0.4,0.6,0.1,0.5]\n",
    "print(cross_entropy(Y,P))\n",
    "# 4.8283137373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Provided solution'''\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the y_i is either one or zero, and there is only one vector of ys, so this is equivalent of having two classes. If we have multiple classes (n of them), the cross entropy is the following:\n",
    "\n",
    "-Σ[i=1, m] Σ[j=1, n] y_i_j \\* ln(p_i_j) \n",
    "\n",
    "y_i_j is 0 or 1, thus we are only adding the events that actually occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "\n",
    "The error function for one point is:\n",
    "\n",
    "-(1-y)\\*ln(1-y_pred) - y\\*ln(y_pred)\n",
    "\n",
    "if y is equal to 1, only the second term remains; if zero only the first term remains. y_pred is the prediction, or probability, of the datapoint being of class '1' [y_pred = p(y=1)] -> according to the model.\n",
    "\n",
    "The error function for the model then is the average of the errors:\n",
    "\n",
    "-1/m \\*  Σ[i=1, m] (1-y_i)\\*ln(1-y_pred_i) + y_i\\*ln(y_pred_i)\n",
    "\n",
    "or, since y_pred = f_act(Wx+b)\n",
    "\n",
    "-1/m \\*  Σ[i=1, m] (1-y_i)\\*ln(1-f_act(Wx_i+b)) + y_i\\*ln(f_act(Wx_i+b))\n",
    "\n",
    "Generalizing for more than two classes:\n",
    "\n",
    "-1/m \\*  Σ[i=1, m] Σ[j=1, n] y_i_j \\* ln(y_pred_i_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "For a point with coordinates (x_1, ... , x_n), label y, and prediction y_pred; the gradient of the error function at that point is:\n",
    "\n",
    "(-(y-y_pred)\\*x_1, ... , -(y-y_pred)\\*x_n), -(y-y_pred)) = -(y-y_pred)\\*(x_1, ... , x_n)\n",
    "\n",
    "### Gradient Descent Step\n",
    "\n",
    "Now lets add the learning rate `alpha`\n",
    "\n",
    "Since w_i - `alpha`(-(y-y_pred)\\*x_i) = w_i + `alpha`(y-y_pred)\\*x_i\n",
    "\n",
    "w_i_new = w_i + (1/m)\\*`alpha`\\*x_i\\*(y-y_pred)\n",
    "\n",
    "and\n",
    "\n",
    "b_new = b + (1/m)\\*`alpha`\\*(y-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Code\n",
    "\n",
    "Again, just for reference:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "def prediction(X, W, b):\n",
    "    return sigmoid(np.matmul(X,W)+b)\n",
    "def error_vector(y, y_hat):\n",
    "    return [-y[i]*np.log(y_hat[i]) - (1-y[i])*np.log(1-y_hat[i]) for i in range(len(y))]\n",
    "def error(y, y_hat):\n",
    "    ev = error_vector(y, y_hat)\n",
    "    return sum(ev)/len(ev)\n",
    "\n",
    "# TODO: Fill in the code below to calculate the gradient of the error function.\n",
    "# The result should be a list of three lists:\n",
    "# The first list should contain the gradient (partial derivatives) with respect to w1\n",
    "# The second list should contain the gradient (partial derivatives) with respect to w2\n",
    "# The third list should contain the gradient (partial derivatives) with respect to b\n",
    "def dErrors(X, y, y_hat):\n",
    "    DErrorsDx1 = [(y[i]-y_hat[i])*X[i][0] for i in range(len(y))]\n",
    "    DErrorsDx2 = [(y[i]-y_hat[i])*X[i][1] for i in range(len(y))]\n",
    "    DErrorsDb = [(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    return DErrorsDx1, DErrorsDx2, DErrorsDb\n",
    "\n",
    "# TODO: Fill in the code below to implement the gradient descent step.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b.\n",
    "# It should calculate the prediction, the gradients, and use them to\n",
    "# update the weights and bias W, b. Then return W and b.\n",
    "# The error e will be calculated and returned for you, for plotting purposes.\n",
    "def gradientDescentStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # TODO: Calculate the prediction\n",
    "    y_hat = prediction(X, W, b)\n",
    "    # TODO: Calculate the gradient\n",
    "    gradient = dErrors(X, y, y_hat)\n",
    "    # TODO: Update the weights\n",
    "    W[0] += learn_rate*sum(gradient[0])\n",
    "    W[1] += learn_rate*sum(gradient[1])\n",
    "    b += learn_rate*sum(gradient[2])\n",
    "    # This calculates the error\n",
    "    e = error(y, y_hat)\n",
    "    return W, b, e\n",
    "\n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainLR(X, y, learn_rate = 0.01, num_epochs = 100):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    # Initialize the weights randomly\n",
    "    W = np.array(np.random.rand(2,1))*2 -1\n",
    "    b = np.random.rand(1)[0]*2 - 1\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    errors = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the gradient descent step.\n",
    "        W, b, error = gradientDescentStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "        errors.append(error)\n",
    "    return boundary_lines, errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Multilayer perceptrons to achieve non linear model from the linear combination of linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916827303506\n",
      "0.880797077978\n",
      "0.802183888559\n"
     ]
    }
   ],
   "source": [
    "# Combination of two perceptrons:\n",
    "\n",
    "def combination(w_1, w_2, b):\n",
    "    return w_1*0.4 + w_2*0.6 + b\n",
    "\n",
    "# Check for combination tha returns probability 0.88:\n",
    "print(sigmoid_f(combination(2, 6, -2)))\n",
    "print(sigmoid_f(combination(3, 5, -2.2)))\n",
    "print(sigmoid_f(combination(5, 4, -3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass classification you need as many outputs as there are classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward and Backpropagation\n",
    "\n",
    "### Feedforward: \n",
    "\n",
    "With matrix of weights from first layer is W_1, from m layer W_m. The prediction y_pred is:\n",
    "\n",
    "y_pred = sigma o W_n o ... sigma o W_1\n",
    "\n",
    "### Backpropagation:\n",
    "\n",
    "Calculate derivatives of the error function using cahin rule:\n",
    "\n",
    "The error function can be seen as a funtion on all the weights of all the layers.\n",
    "\n",
    "E(W) = E(w1_1, w1_2, ... wm_1, ... wm_n)\n",
    "\n",
    "Therefore the gradient is the vector fomed by all the partial derivatives:\n",
    "\n",
    "∇E = (∂E/∂w1_1 ... ∂E/∂wm_n)\n",
    "\n",
    "Where each component is the product of all partial derivatives. Also, having that the derivative of the sigmoid function is:\n",
    "\n",
    "σ'(x) = σ(x)(1 - σ(x))\n",
    "\n",
    "A given partial derivative is just the corresponding weight multiplied by the expression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
