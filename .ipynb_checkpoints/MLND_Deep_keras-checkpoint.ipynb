{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in Keras\n",
    "\n",
    "Personal notes on Udacity MLND Deep Learning section.\n",
    "\n",
    "Let's start with a singel hidden layer model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 129\n",
      "Trainable params: 129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28675633668899536, 1.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "\n",
    "# X shape: (num_rows, num_cols)\n",
    "# training data stored as row vectors\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "\n",
    "# y must have an output vector for each input vector\n",
    "y = np.array([[0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "'''\n",
    "Create sequential model.\n",
    "This is a wraper to treat the network as a sequence of layers that implements the model interface with\n",
    "methods `compile()`, `fit()`, `evaluate()`... used to train and run the model.\n",
    "'''\n",
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "The Layer class provides common interface for a variety of layers (fully connected, max pool, activation, ...)\n",
    "You can add a layer to a model using the model's `add()` method\n",
    "'''\n",
    "# 1st Layer - Add an input layer of 32 nodes with the same input shape as the training samples in X\n",
    "# you only have to explicitly set the input dimensions for the first layer\n",
    "model.add(Dense(32, input_dim=X.shape[1]))\n",
    "\n",
    "# Add a softmax activation layer\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 2nd Layer - Add a fully connected output layer (1 node)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Add a sigmoid activation layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "'''\n",
    "Compiling\n",
    "Call backend (TF), and bind loss function, optimizer, evaluation metric, ...\n",
    "'''\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "'''\n",
    "See the resulting model architecture\n",
    "'''\n",
    "model.summary()\n",
    "\n",
    "'''\n",
    "Train the model\n",
    "'''\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Stochastic Gradient Descent, specify batches:\n",
    "##model.fit(X_train, y_train, epochs=1000, batch_size=100, verbose=0)\n",
    "\n",
    "'''\n",
    "Evaluate the model\n",
    "'''\n",
    "model.evaluate(X, y) # score = model.evaluate(x_test, y_test, verbose=0) \n",
    "                     # https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xor quiz\n",
    "\n",
    "Code used in the interface:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "# Using TensorFlow 1.0.0; use tf.python_io in later versions\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "\n",
    "# Add required layers\n",
    "xor.add(Dense(8, input_dim=2))\n",
    "xor.add(Activation('tanh'))\n",
    "xor.add(Dense(1))\n",
    "xor.add(Activation('sigmoid'))\n",
    "\n",
    "# Specify loss as \"binary_crossentropy\", optimizer as \"adam\",\n",
    "# and add the accuracy metric\n",
    "xor.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, nb_epoch=300, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "Using TensorFlow backend.\n",
    "____________________________________________________________________________________________________\n",
    "Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "====================================================================================================\n",
    "dense_1 (Dense)                  (None, 8)             24          dense_input_1[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "activation_1 (Activation)        (None, 8)             0           dense_1[0][0]                    \n",
    "____________________________________________________________________________________________________\n",
    "dense_2 (Dense)                  (None, 1)             9           activation_1[0][0]               \n",
    "____________________________________________________________________________________________________\n",
    "activation_2 (Activation)        (None, 1)             0           dense_2[0][0]                    \n",
    "====================================================================================================\n",
    "Total params: 33\n",
    "Trainable params: 33\n",
    "Non-trainable params: 0\n",
    "____________________________________________________________________________________________________\n",
    "\n",
    "4/4 [==============================] - 0s\n",
    "\n",
    "Accuracy:  1.0\n",
    "\n",
    "Predictions:\n",
    "\n",
    "4/4 [==============================] - 0s\n",
    "[[ 0.42648906]\n",
    " [ 0.51158553]\n",
    " [ 0.57282132]\n",
    " [ 0.48525804]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To split in test and training sets:\n",
    "\n",
    "```\n",
    "# Test and training sets example\n",
    "\n",
    "(X_train, X_test) = X[50:], X[:50]\n",
    "(y_train, y_test) = y[50:], y[:50]\n",
    "```\n",
    "\n",
    "## Overfiting Management\n",
    "\n",
    "Underfitting => High bias\n",
    "Overfitting => High variance\n",
    "\n",
    "Go for overfitting and then try to reduce it:\n",
    "\n",
    "### 1. Stop early  \n",
    "Right number of epochs: plot epochs vs training / testing error [Model Complexity Graph]\n",
    "\n",
    "### 2. Regularization\n",
    "Large coefficients tend to overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 first prediction: 0.880797077978\n",
      "Model 1 second prediction: 0.119202922022\n",
      "Model 2 first prediction: 0.999999997939\n",
      "Model 2 second prediction: 2.06115361819e-09\n"
     ]
    }
   ],
   "source": [
    "# Large coefficient example\n",
    "\n",
    "'''\n",
    "Separate (1,1) and (-1,-1)\n",
    "first model: x_1 + x_2 => w_1=1, w_2=1\n",
    "second model: 10x_1 + 10x_2 => w_1=10, w_2=10\n",
    "'''\n",
    "\n",
    "# apply sigmoid function\n",
    "from scipy.special import expit\n",
    "\n",
    "# Model 1\n",
    "print('Model 1 first prediction:', expit(1+1)) # (1,1) prediction\n",
    "print('Model 1 second prediction:', expit(-1-1)) # (-1,-1) prediction\n",
    "\n",
    "# Model 2\n",
    "print('Model 2 first prediction:', expit(10+10)) # (1,1) prediction\n",
    "print('Model 2 second prediction:', expit(-10-10)) # (-1,-1) prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model is overffiting.\n",
    "\n",
    "To penalize large weights one can add a term in the error function that is the sums of the absolute values of the weights [L1 Regularization], or the sum of the squares of the weights [L2 Regularization], in any case times a constant lambda. \n",
    "\n",
    "L1 yields sparse vectors (small vectors go to 0). Good for feature selection.\n",
    "\n",
    "L2 tries to mantain all the weights homogeneously small. Normally better for training models.\n",
    "\n",
    "### 3. Dropout\n",
    "\n",
    "As training goes through epochs, randomly turn off some of the nodes. To do this we give the algorithm a new parameter for the nodes. This parameter is the probability that a node gets dropped at a given epoch.\n",
    "\n",
    "Applying dropout in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 2.3505 - acc: 0.0920\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 2.3112 - acc: 0.1180\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.3259 - acc: 0.1020\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.3024 - acc: 0.1090\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3019 - acc: 0.1180\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3040 - acc: 0.1130\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 2.3014 - acc: 0.1190\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.3045 - acc: 0.1110\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.2957 - acc: 0.1320\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.3014 - acc: 0.1070\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.2956 - acc: 0.1150\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.2959 - acc: 0.1210\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.2898 - acc: 0.1170\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.2876 - acc: 0.1360\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.2937 - acc: 0.1160\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.2944 - acc: 0.1310\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.2801 - acc: 0.1340\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.2917 - acc: 0.1270\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.2892 - acc: 0.1380\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.2884 - acc: 0.1420\n",
      "100/100 [==============================] - 0s 550us/step\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ref: https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "'''\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model2 = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model2.add(Dense(64, activation='relu', input_dim=20))\n",
    "model2.add(Dropout(0.2)) # After adding layers, add dropout\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.2)) # In each epoch, each node gets dropped off with a probability of 0.2\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model2.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient\n",
    "\n",
    "The sigmoid function gets flat at both ends, and its derivatives tend to zero on both directions. using the chain rule this tendency exacerbates. Leave output as sigmoid (for probability), and change activation functions in other layers.\n",
    "\n",
    "Alternative activation functions (Keras):\n",
    "\n",
    "```\n",
    "model.add(Activation('relu'))\n",
    "model.add(Activation('tanh'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Minima\n",
    "\n",
    "1. Use random restart.\n",
    "2. Use momentum [beta], a constant from 0 to 1: the average of last weighted steps (last step matters most):\n",
    "step(n) + beta * step(n-1) + beta^2 * step(n-2) + ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Optimizers\n",
    "\n",
    "Ref:\n",
    "\n",
    "[Optimizers - Keras Docs](https://keras.io/optimizers/)\n",
    "\n",
    "[Gradient Descent optimization algos](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop)\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "Parameters: learning rate, momentum, nesterov momentum (slows down gradient when close to solution)\n",
    "\n",
    "### Adam\n",
    "Adaptive Moment Estimation. Uses exponential decay that consists of the average (first moment), and the variance (second moment) of the previous steps.\n",
    "\n",
    "### RMSProp\n",
    "Root Mean Squared Error. Decreases the learning rate by dividing it by an exponentially decaying average of squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
